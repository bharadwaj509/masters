{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn.manifold\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import gensim.models.word2vec as w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "safdas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryan/anaconda/lib/python3.6/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['seed']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "print(\"safdas\")\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aryan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/aryan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = r'annotations'  # use your path\n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "raw_sentences = []\n",
    "\n",
    "for file in allFiles:\n",
    "\n",
    "    dictCollection = {\"Action First-Party\":[],\n",
    "                      \"Purpose\":[],\n",
    "                      \"Personal Information Type\":[]\n",
    "                      }\n",
    "\n",
    "    df = pd.read_csv(file, thousands=',', header=None)\n",
    "    len(df)\n",
    "    # df_tail = df.tail(1)[4]\n",
    "    # print(df_tail)\n",
    "    number_of_segments = len(df) + 1\n",
    "\n",
    "    file = file.split('annotations/', 1)[1]\n",
    "\n",
    "    myFile = open('collections/' + file, 'a', newline='')\n",
    "    wr = csv.writer(myFile)\n",
    "    # print(file)\n",
    "    # print(number_of_segments)\n",
    "    for i in range(number_of_segments-1):\n",
    "        # print(i)\n",
    "        if df[5][i] == \"First Party Collection/Use\":\n",
    "            parse_json = json.loads(str(df[6][i]))\n",
    "            if parse_json[\"Action First-Party\"][\"endIndexInSegment\"] != -1:\n",
    "                raw_sentences.append(parse_json[\"Action First-Party\"][\"selectedText\"])\n",
    "            if parse_json[\"Personal Information Type\"][\"endIndexInSegment\"] != -1:\n",
    "                raw_sentences.append(parse_json[\"Personal Information Type\"][\"selectedText\"])\n",
    "            if parse_json[\"Purpose\"][\"endIndexInSegment\"] != -1:\n",
    "                raw_sentences.append(parse_json[\"Purpose\"][\"selectedText\"])\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other purposes, for example when you report a problem with our site\n",
      "['other', 'purposes', 'for', 'example', 'when', 'you', 'report', 'a', 'problem', 'with', 'our', 'site']\n"
     ]
    }
   ],
   "source": [
    "print(raw_sentences[5])\n",
    "print(sentence_to_wordlist(raw_sentences[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 216,323 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The book corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 300\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "#more workers, faster we train\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "context_size = 7\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "#0 - 1e-5 is good for this\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "#random number generator\n",
    "#deterministic, good for debugging\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-08 11:01:35,639 : INFO : collecting all words and their counts\n",
      "2017-02-08 11:01:35,641 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-02-08 11:01:35,667 : INFO : PROGRESS: at sentence #10000, processed 101519 words, keeping 2585 word types\n",
      "2017-02-08 11:01:35,695 : INFO : PROGRESS: at sentence #20000, processed 201786 words, keeping 3634 word types\n",
      "2017-02-08 11:01:35,701 : INFO : collected 3813 word types from a corpus of 216323 raw words and 21598 sentences\n",
      "2017-02-08 11:01:35,702 : INFO : Loading a fresh vocabulary\n",
      "2017-02-08 11:01:35,712 : INFO : min_count=3 retains 2750 unique words (72% of original 3813, drops 1063)\n",
      "2017-02-08 11:01:35,714 : INFO : min_count=3 leaves 214806 word corpus (99% of original 216323, drops 1517)\n",
      "2017-02-08 11:01:35,726 : INFO : deleting the raw counts dictionary of 3813 items\n",
      "2017-02-08 11:01:35,728 : INFO : sample=0.001 downsamples 67 most-common words\n",
      "2017-02-08 11:01:35,730 : INFO : downsampling leaves estimated 148758 word corpus (69.3% of prior 214806)\n",
      "2017-02-08 11:01:35,731 : INFO : estimated required memory for 2750 words and 300 dimensions: 7975000 bytes\n",
      "2017-02-08 11:01:35,743 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "policy2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-08 11:01:39,123 : WARNING : direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 2750\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec vocabulary length:\", len(policy2vec.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-08 11:01:41,676 : INFO : training model with 8 workers on 2750 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=7\n",
      "2017-02-08 11:01:41,677 : INFO : expecting 21598 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-08 11:01:42,704 : INFO : PROGRESS: at 51.77% examples, 379141 words/s, in_qsize 15, out_qsize 0\n",
      "2017-02-08 11:01:43,462 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-02-08 11:01:43,503 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-02-08 11:01:43,507 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-02-08 11:01:43,521 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-02-08 11:01:43,534 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-02-08 11:01:43,553 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-08 11:01:43,557 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-08 11:01:43,561 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-08 11:01:43,562 : INFO : training on 1081615 raw words (743676 effective words) took 1.9s, 397050 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "743676"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy2vec.train(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('area', 0.829443097114563),\n",
       " ('location', 0.8279428482055664),\n",
       " ('GPS', 0.8117223381996155),\n",
       " ('entertainment', 0.8099267482757568),\n",
       " ('audiences', 0.8019210696220398),\n",
       " ('representative', 0.7787646055221558),\n",
       " ('current', 0.7669483423233032),\n",
       " ('agent', 0.7573494911193848),\n",
       " ('general', 0.7568982839584351),\n",
       " ('characteristics', 0.7485439777374268)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy2vec.most_similar(\"demographic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-08 11:08:28,546 : WARNING : direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n"
     ]
    }
   ],
   "source": [
    "word2vec_dict={}\n",
    "for i in policy2vec.vocab.keys():\n",
    "    try:\n",
    "        word2vec_dict[i]=policy2vec[i]\n",
    "    except:    \n",
    "        pass\n",
    "\n",
    "\n",
    "clusters = MiniBatchKMeans(n_clusters=100, max_iter=10,batch_size=200,\n",
    "                        n_init=1,init_size=2000)\n",
    "X = np.array([i.T for i in word2vec_dict.values()])\n",
    "y = [i for i in word2vec_dict.keys()]\n",
    "clusters.fit(X)\n",
    "from collections import defaultdict\n",
    "cluster_dict=defaultdict(list)\n",
    "for word,label in zip(y,clusters.labels_):\n",
    "    cluster_dict[label].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['administer', 'aggregate', 'analyses', 'analyze', 'around', 'base', 'broad', 'demographic', 'gather', 'measure', 'movement', 'movements', 'patterns', 'statistical', 'track', 'traffic', 'trends', 'usage']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cluster_dict)):\n",
    "    if 'demographic' in cluster_dict[i]:\n",
    "        cluster_dict[i].sort()\n",
    "        print(cluster_dict[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
