{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "66\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "path = r'annotations'  # use your path\n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "data_samples1 = []\n",
    "data_number1 = []\n",
    "data_label1 = []\n",
    "\n",
    "cnt = 0\n",
    "cnt1 = 0\n",
    "cnt2 = 0\n",
    "cnt3 = 0\n",
    "\n",
    "for file in allFiles:\n",
    "#     allFiles = allFiles[:85]\n",
    "    dictCollection = {\"Action First-Party\":[],\n",
    "                      \"Purpose\":[],\n",
    "                      \"Personal Information Type\":[]\n",
    "                      }\n",
    "\n",
    "    df = pd.read_csv(file, thousands=',', header=None)\n",
    "    len(df)\n",
    "    # df_tail = df.tail(1)[4]\n",
    "    # print(df_tail)\n",
    "    number_of_segments = 5#len(df) + 1\n",
    "\n",
    "    file = file.split('annotations/', 1)[1]\n",
    "#     print(file)\n",
    "    for i in range(number_of_segments-1):\n",
    "        # print(i)\n",
    "        if df[5][i] == \"First Party Collection/Use\":\n",
    "            parse_json = json.loads(str(df[6][i]))\n",
    "#             print(parse_json)\n",
    "            if parse_json[\"Action First-Party\"][\"endIndexInSegment\"] != -1:\n",
    "                data_samples1.append(parse_json[\"Action First-Party\"][\"selectedText\"])\n",
    "                data_number1.append(1)\n",
    "                data_label1.append(\"Action First-Party\")\n",
    "                cnt1 = cnt1+1\n",
    "            if parse_json[\"Personal Information Type\"][\"endIndexInSegment\"] != -1:\n",
    "                data_samples1.append(parse_json[\"Personal Information Type\"][\"selectedText\"])\n",
    "                data_number1.append(2)\n",
    "                data_label1.append(\"Other\")                \n",
    "                cnt2 = cnt2+1\n",
    "            if parse_json[\"Purpose\"][\"endIndexInSegment\"] != -1:\n",
    "                data_samples1.append(parse_json[\"Purpose\"][\"selectedText\"])\n",
    "                data_number1.append(2)\n",
    "                data_label1.append(\"Other\")     \n",
    "                cnt3 = cnt3+1\n",
    "print(cnt1)\n",
    "print(cnt2)\n",
    "print(cnt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Action First-Party', 'Personal Information Type', 'Purpose', 'Action First-Party', 'Personal Information Type']\n",
      "167\n"
     ]
    }
   ],
   "source": [
    "print(data_label[:5])\n",
    "print(len(data_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 1, 2]\n",
      "167\n"
     ]
    }
   ],
   "source": [
    "print(data_number[:5])\n",
    "print(len(data_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sci-News.com may collect and process', 'including', 'other purposes, for example when you report a problem with our site', 'Sci-News.com may collect and process', 'names, e-mail and website addresses']\n",
      "167\n",
      "['Sci-News.com may collect and process' 'including'\n",
      " 'other purposes, for example when you report a problem with our site'\n",
      " 'Sci-News.com may collect and process'\n",
      " 'names, e-mail and website addresses'\n",
      " 'other purposes, for example when you report a problem with our site'\n",
      " 'collects information from our users at several different points on our Web site.'\n",
      " 'information' 'we collect and receive' 'information'\n",
      " \"We're always working to make our services even better. One of the ways we do that is by analyzing information\"\n",
      " 'information that you choose to publicly post to or display on a Site.'\n",
      " 'to information that you choose to publicly post to or display on a Site'\n",
      " 'This privacy policy does not apply to information that you choose to publicly post to or display on a Site.'\n",
      " 'when you subscribe to one of our magazines or place an order'\n",
      " 'names, mailing addresses, e-mail addresses'\n",
      " 'when you subscribe to one of our magazines or place an order'\n",
      " 'information, such as'\n",
      " 'when you subscribe to one of our magazines or place an order'\n",
      " 'names, mailing addresses, e-mail addresses'\n",
      " 'each time you visit the Site, we automatically collect' 'IP address'\n",
      " 'to administer and optimize the Site for you'\n",
      " 'email address, name, postal address, and phone number.' 'collect'\n",
      " 'personal information'\n",
      " 'Your privacy is important to us. Public Citizen does not automatically collect personal information from people who visit or interact with our website.'\n",
      " 'on the website, we may store'\n",
      " 'your name, postal address, email address, phone number'\n",
      " 'contact you about our work' 'Site' 'Protocol' 'collects'\n",
      " 'personally identifiable information' 'we will place one or mor'\n",
      " '\"cookies\"'\n",
      " 'When you use Bing and MSN services with a web browser, we will place one or more \"cookies\" on your machine.'\n",
      " 'Bing uses' 'cookie'\n",
      " 'to operate the service and enable certain search features'\n",
      " 'collect information from users of the Websites' 'information'\n",
      " \"This Privacy Statement addresses SRL's practices regarding information collected only directly through or from the Site\"\n",
      " 'information'\n",
      " \"This Privacy Statement addresses SRL's practices regarding information collected only directly through or from the Site - it does not address or govern any information gathering, use, or dissemination practices related to information collected other than directly through or from the Site, including, without limitation, from or via telephone, facsimile, postal mail, personal delivery, or other or additional offline means or media. SRL can be contacted by mail, phone, facsimile, or by e-mail at SRL, as provided in Paragraph 3 of this Privacy Statement.\"\n",
      " 'a mobile device or any other technology or devices now known or hereafter developed or discovered (each, a \"Device\")'\n",
      " 'collect' 'information' 'facilitate and improve our services' 'digital'\n",
      " 'your name, birthday, email address, city of residence, ZIP code'\n",
      " \"determine our users' preferences, to design initiatives to meet those preferences\"\n",
      " 'digital' 'birthday'\n",
      " \"determine our users' preferences, to design initiatives to meet those preferences\"\n",
      " 'visit the Washingtonian.com site.'\n",
      " 'information (your name, e-mail address, street address, telephone number)'\n",
      " 'when you subscribe to Washingtonian magazine'\n",
      " 'information (your name, e-mail address, street address, telephone number)'\n",
      " 'On certain pages, however, you may be asked to provide'\n",
      " 'personal information'\n",
      " 'subscribe to Washingtonian Magazine, renew your subscription, purchase a holiday gift subscription, submit a customer service inquiry, or make a purchase'\n",
      " 'On certain pages, however, you may be asked to provide'\n",
      " 'personal information'\n",
      " 'if you participate in sweepstakes and contests, surveys, message boards and chat rooms, or other interactive areas of our site.'\n",
      " 'nformation' 'By accessing the Sites,'\n",
      " 'such as when you provide your name, contact information'\n",
      " 'for such purposes as purchasing products, creating an account, joining our loyalty programs, filling out a survey, entering contests, participating in promotions or electronic activities, posting comments on bulletin boards or otherwise interacting with our Sites.'\n",
      " 'when you visit our web site' 'personal information'\n",
      " 'When you visit USA.gov' 'the Internet protocol address'\n",
      " 'we collect from users who visit and submit applications to our website'\n",
      " 'information' 'how we use and disclose that information.'\n",
      " 'obtained from our online visitors' 'personal information'\n",
      " 'intended purpose' 'personal information we collect' 'may include'\n",
      " 'allows us to contact you to provide a service or carry out a transaction that you have requested such as receiving information about Abita products and services, entering a contest, ordering e-mail newsletters, joining a limited-access premium site or service, or when purchasing, downloading and/or registering Abita products'\n",
      " 'Web site' 'IP' 'administer' 'In our site' 'name, address, email'\n",
      " 'give you a more personalized shopping experience' 'In our site'\n",
      " 'name, address, email' 'make ordering and reordering easier for you'\n",
      " 'In our site' 'other relevant information'\n",
      " 'give you a more personalized shopping experience' 'In our site'\n",
      " 'other relevant information' 'make ordering and reordering easier for you'\n",
      " 'By submitting PersonallyIdentifiable Information through any of our Covered Sites,'\n",
      " 'name, email address, address,telephone number'\n",
      " 'When you share information with us' 'information'\n",
      " 'or example by creating a Google Account' 'we may request'\n",
      " 'name, zip code and telephone number'\n",
      " 'additional services as well as more interesting and relevant content'\n",
      " 'we may request' 'e-mail address'\n",
      " 'In order to distribute our email publication' 'we may request'\n",
      " 'e-mail address' 'more interesting and relevant content'\n",
      " 'when you join one of our loyalty clubs, enter one of our surveys, contests or sweepstakes, if you submit a question, comment or complaint, if you request or submit information about becoming a franchisee, if you submit an employment application, or for an online cake order.'\n",
      " 'such as'\n",
      " 'This Privacy Policy applies to information we collect about you from any source - through our Websites, in our retail stores, through our Power-Up Rewards Program, in any contest or promotion, or any other way.'\n",
      " 'information we collect about you from any source'\n",
      " 'Any and all personal identifiable information collected from our customers'\n",
      " 'personal identifiable information' 'is used by Tanger Outlets only,'\n",
      " 'collect or use' 'your personal information']\n"
     ]
    }
   ],
   "source": [
    "print(data_samples[:5])\n",
    "print(len(data_samples))\n",
    "\n",
    "train_samples = np.array(data_samples[:120])\n",
    "train_label = np.array(data_label[:120])\n",
    "train_number = np.array(data_number[:120])\n",
    "\n",
    "test_samples = np.array(data_samples[121:167])\n",
    "test_label = np.array(data_label[121:167])\n",
    "test_number = np.array(data_number[121:167])\n",
    "print(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'do',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=1000)\n",
    "X_train_counts = count_vect.fit_transform(train_samples)\n",
    "X_train_counts\n",
    "count_vect.get_feature_names()\n",
    "count_vect.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get('address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 292)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 292)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "text_clf = text_clf.fit(train_samples, train_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71739130434782605"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(test_samples)\n",
    "np.mean(predicted == test_number)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71739130434782605"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf1 = Pipeline([('vect', CountVectorizer()), \n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, n_iter=5, random_state=42))])\n",
    "_ = text_clf1.fit(train_samples, train_number)\n",
    "predicted1 = text_clf1.predict(test_samples)\n",
    "np.mean(predicted1 == test_number)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           precision    recall  f1-score   support\n",
      "\n",
      "       Action First-Party       0.67      0.40      0.50        15\n",
      "Personal Information Type       0.95      0.90      0.92        20\n",
      "                  Purpose       0.50      0.82      0.62        11\n",
      "\n",
      "              avg / total       0.75      0.72      0.71        46\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 6,  1,  8],\n",
       "       [ 1, 18,  1],\n",
       "       [ 2,  0,  9]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(test_number, predicted,\n",
    "                                        target_names=test_label))\n",
    "\n",
    "metrics.confusion_matrix(test_number, predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sci-News.com may collect and process', 'including', 'other purposes, for example when you report a problem with our site', 'Sci-News.com may collect and process', 'names, e-mail and website addresses']\n",
      "167\n",
      "['Sci-News.com may collect and process' 'including'\n",
      " 'other purposes, for example when you report a problem with our site'\n",
      " 'Sci-News.com may collect and process'\n",
      " 'names, e-mail and website addresses'\n",
      " 'other purposes, for example when you report a problem with our site'\n",
      " 'collects information from our users at several different points on our Web site.'\n",
      " 'information' 'we collect and receive' 'information'\n",
      " \"We're always working to make our services even better. One of the ways we do that is by analyzing information\"\n",
      " 'information that you choose to publicly post to or display on a Site.'\n",
      " 'to information that you choose to publicly post to or display on a Site'\n",
      " 'This privacy policy does not apply to information that you choose to publicly post to or display on a Site.'\n",
      " 'when you subscribe to one of our magazines or place an order'\n",
      " 'names, mailing addresses, e-mail addresses'\n",
      " 'when you subscribe to one of our magazines or place an order'\n",
      " 'information, such as'\n",
      " 'when you subscribe to one of our magazines or place an order'\n",
      " 'names, mailing addresses, e-mail addresses'\n",
      " 'each time you visit the Site, we automatically collect' 'IP address'\n",
      " 'to administer and optimize the Site for you'\n",
      " 'email address, name, postal address, and phone number.' 'collect'\n",
      " 'personal information'\n",
      " 'Your privacy is important to us. Public Citizen does not automatically collect personal information from people who visit or interact with our website.'\n",
      " 'on the website, we may store'\n",
      " 'your name, postal address, email address, phone number'\n",
      " 'contact you about our work' 'Site' 'Protocol' 'collects'\n",
      " 'personally identifiable information' 'we will place one or mor'\n",
      " '\"cookies\"'\n",
      " 'When you use Bing and MSN services with a web browser, we will place one or more \"cookies\" on your machine.'\n",
      " 'Bing uses' 'cookie'\n",
      " 'to operate the service and enable certain search features'\n",
      " 'collect information from users of the Websites' 'information'\n",
      " \"This Privacy Statement addresses SRL's practices regarding information collected only directly through or from the Site\"\n",
      " 'information'\n",
      " \"This Privacy Statement addresses SRL's practices regarding information collected only directly through or from the Site - it does not address or govern any information gathering, use, or dissemination practices related to information collected other than directly through or from the Site, including, without limitation, from or via telephone, facsimile, postal mail, personal delivery, or other or additional offline means or media. SRL can be contacted by mail, phone, facsimile, or by e-mail at SRL, as provided in Paragraph 3 of this Privacy Statement.\"\n",
      " 'a mobile device or any other technology or devices now known or hereafter developed or discovered (each, a \"Device\")'\n",
      " 'collect' 'information' 'facilitate and improve our services' 'digital'\n",
      " 'your name, birthday, email address, city of residence, ZIP code'\n",
      " \"determine our users' preferences, to design initiatives to meet those preferences\"\n",
      " 'digital' 'birthday'\n",
      " \"determine our users' preferences, to design initiatives to meet those preferences\"\n",
      " 'visit the Washingtonian.com site.'\n",
      " 'information (your name, e-mail address, street address, telephone number)'\n",
      " 'when you subscribe to Washingtonian magazine'\n",
      " 'information (your name, e-mail address, street address, telephone number)'\n",
      " 'On certain pages, however, you may be asked to provide'\n",
      " 'personal information'\n",
      " 'subscribe to Washingtonian Magazine, renew your subscription, purchase a holiday gift subscription, submit a customer service inquiry, or make a purchase'\n",
      " 'On certain pages, however, you may be asked to provide'\n",
      " 'personal information'\n",
      " 'if you participate in sweepstakes and contests, surveys, message boards and chat rooms, or other interactive areas of our site.'\n",
      " 'nformation' 'By accessing the Sites,'\n",
      " 'such as when you provide your name, contact information'\n",
      " 'for such purposes as purchasing products, creating an account, joining our loyalty programs, filling out a survey, entering contests, participating in promotions or electronic activities, posting comments on bulletin boards or otherwise interacting with our Sites.'\n",
      " 'when you visit our web site' 'personal information'\n",
      " 'When you visit USA.gov' 'the Internet protocol address'\n",
      " 'we collect from users who visit and submit applications to our website'\n",
      " 'information' 'how we use and disclose that information.'\n",
      " 'obtained from our online visitors' 'personal information'\n",
      " 'intended purpose' 'personal information we collect' 'may include'\n",
      " 'allows us to contact you to provide a service or carry out a transaction that you have requested such as receiving information about Abita products and services, entering a contest, ordering e-mail newsletters, joining a limited-access premium site or service, or when purchasing, downloading and/or registering Abita products'\n",
      " 'Web site' 'IP' 'administer' 'In our site' 'name, address, email'\n",
      " 'give you a more personalized shopping experience' 'In our site'\n",
      " 'name, address, email' 'make ordering and reordering easier for you'\n",
      " 'In our site' 'other relevant information'\n",
      " 'give you a more personalized shopping experience' 'In our site'\n",
      " 'other relevant information' 'make ordering and reordering easier for you'\n",
      " 'By submitting PersonallyIdentifiable Information through any of our Covered Sites,'\n",
      " 'name, email address, address,telephone number'\n",
      " 'When you share information with us' 'information'\n",
      " 'or example by creating a Google Account' 'we may request'\n",
      " 'name, zip code and telephone number'\n",
      " 'additional services as well as more interesting and relevant content'\n",
      " 'we may request' 'e-mail address'\n",
      " 'In order to distribute our email publication' 'we may request'\n",
      " 'e-mail address' 'more interesting and relevant content'\n",
      " 'when you join one of our loyalty clubs, enter one of our surveys, contests or sweepstakes, if you submit a question, comment or complaint, if you request or submit information about becoming a franchisee, if you submit an employment application, or for an online cake order.'\n",
      " 'such as'\n",
      " 'This Privacy Policy applies to information we collect about you from any source - through our Websites, in our retail stores, through our Power-Up Rewards Program, in any contest or promotion, or any other way.'\n",
      " 'information we collect about you from any source'\n",
      " 'Any and all personal identifiable information collected from our customers'\n",
      " 'personal identifiable information' 'is used by Tanger Outlets only,'\n",
      " 'collect or use' 'your personal information'\n",
      " 'Dog Breed Info Center does not collect or use your personal information,'\n",
      " 'collect or use' 'your personal information' 'n limited circumstances.'\n",
      " 'may collect'\n",
      " 'your name, email address, home address and/or phone number.'\n",
      " 'When submitting photos, listing in our classifieds, or taking our survey'\n",
      " 'visitor to our Web site'\n",
      " 'domain name, operating system, and web browser version'\n",
      " 'make your return visits to a Web site more convenient'\n",
      " 'register on our website or participate in our offers and programs'\n",
      " 'personally-identifiable information, such as'\n",
      " 'certain additional features that allow you to receive or access your favorite recipes and participate in our interactive communities.'\n",
      " 'register on our website or participate in our offers and programs'\n",
      " 'personally-identifiable information, such as'\n",
      " 'Sharing this additional information with us also helps us customize your website experience.'\n",
      " 'register on our website or participate in our offers and programs'\n",
      " 'personally-identifiable information, such as'\n",
      " 'In addition, we may use the information you provide us to send you offers and information about Kraft, its portfolio of brands and its partners.'\n",
      " 'personal information' 'personal information'\n",
      " 'The Smithsonian will use that information to respond to your message and to help us get you the information you have requested.'\n",
      " 'may be intercepted, monitored, recorded, copied, audited, inspected, and disclosed'\n",
      " 'all files'\n",
      " 'em you indicate your awareness of and consent to these terms and conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions stated in this warning.'\n",
      " 'is collected' 'information'\n",
      " 'it will be used solely in connection with Jefferson Lab or for such other purposes as are described at the point of collection'\n",
      " 'personal information'\n",
      " 'We need this information to process payment and ship merchandise to you.']\n"
     ]
    }
   ],
   "source": [
    "print(data_samples[:5])\n",
    "print(len(data_samples))\n",
    "\n",
    "train_samples1 = np.array(data_samples[:150])\n",
    "train_label1 = np.array(data_label[:150])\n",
    "train_number1 = np.array(data_number[:150])\n",
    "\n",
    "test_samples1 = np.array(data_samples[151:167])\n",
    "test_label1 = np.array(data_label[151:167])\n",
    "test_number1 = np.array(data_number[151:167])\n",
    "print(train_samples1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'access', 'account', 'additional', 'address', 'addresses', 'administer', 'all', 'an', 'and', 'any', 'as', 'asked', 'at', 'automatically', 'be', 'bing', 'birthday', 'boards', 'browser', 'by', 'certain', 'choose', 'code', 'collect', 'collected', 'collects', 'com', 'contact', 'content', 'contest', 'contests', 'cookies', 'creating', 'design', 'determine', 'digital', 'directly', 'display', 'do', 'does', 'each', 'easier', 'email', 'entering', 'example', 'experience', 'features', 'for', 'from', 'give', 'have', 'however', 'identifiable', 'if', 'in', 'including', 'information', 'initiatives', 'interactive', 'interesting', 'ip', 'is', 'it', 'joining', 'limited', 'loyalty', 'magazine', 'magazines', 'mail', 'mailing', 'make', 'may', 'meet', 'message', 'more', 'name', 'names', 'news', 'not', 'number', 'of', 'offers', 'on', 'one', 'online', 'only', 'or', 'order', 'ordering', 'other', 'our', 'out', 'pages', 'participate', 'personal', 'personalized', 'personally', 'phone', 'place', 'policy', 'post', 'postal', 'practices', 'preferences', 'privacy', 'problem', 'process', 'products', 'programs', 'protocol', 'provide', 'publicly', 'purchasing', 'purposes', 'receive', 'regarding', 'register', 'relevant', 'reordering', 'report', 'request', 'requested', 'sci', 'service', 'services', 'shopping', 'site', 'sites', 'source', 'srl', 'statement', 'street', 'submit', 'submitting', 'subscribe', 'such', 'survey', 'surveys', 'sweepstakes', 'telephone', 'that', 'the', 'this', 'those', 'through', 'to', 'us', 'use', 'used', 'users', 'visit', 'washingtonian', 'we', 'web', 'website', 'websites', 'when', 'who', 'will', 'with', 'you', 'your', 'zip']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect1 = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=1000)\n",
    "X_train_counts1 = count_vect1.fit_transform(train_samples1)\n",
    "X_train_counts1\n",
    "print(count_vect1.get_feature_names())\n",
    "count_vect1.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 164)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer1 = TfidfTransformer(use_idf=False).fit(X_train_counts1)\n",
    "X_train_tf1 = tf_transformer1.transform(X_train_counts1)\n",
    "X_train_tf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 164)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer1 = TfidfTransformer()\n",
    "X_train_tfidf1 = tfidf_transformer1.fit_transform(X_train_counts1)\n",
    "X_train_tfidf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "text_clf1 = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "text_clf1 = text_clf1.fit(train_samples1, train_number1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf1.predict(test_samples1)\n",
    "np.mean(predicted == test_number1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf1 = Pipeline([('vect', CountVectorizer()), \n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, n_iter=5, random_state=42))])\n",
    "_ = text_clf1.fit(train_samples1, train_number1)\n",
    "predicted = text_clf1.predict(test_samples1)\n",
    "np.mean(predicted == test_number1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167, 178)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect1 = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=1000)\n",
    "X_train_counts1 = count_vect1.fit_transform(data_samples)\n",
    "X_train_counts1\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer1 = TfidfTransformer(use_idf=False).fit(X_train_counts1)\n",
    "X_train_tf1 = tf_transformer1.transform(X_train_counts1)\n",
    "X_train_tf1.shape\n",
    "\n",
    "tfidf_transformer1 = TfidfTransformer()\n",
    "X_train_tfidf1 = tfidf_transformer1.fit_transform(X_train_counts1)\n",
    "X_train_tfidf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92215568862275454"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "text_clf1 = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "text_clf1 = text_clf1.fit(data_samples, data_number)\n",
    "\n",
    "import numpy as np\n",
    "predicted = text_clf1.predict(data_samples)\n",
    "np.mean(predicted == data_number)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9880239520958084"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf1 = Pipeline([('vect', CountVectorizer()), \n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, n_iter=5, random_state=42))])\n",
    "_ = text_clf1.fit(data_samples, data_number)\n",
    "\n",
    "import numpy as np\n",
    "predicted = text_clf1.predict(data_samples)\n",
    "np.mean(predicted == data_number)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1017_sci-news.com.csv\n",
      "1028_redorbit.com.csv\n",
      "1034_aol.com.csv\n",
      "1050_honda.com.csv\n",
      "105_amazon.com.csv\n",
      "1070_wnep.com.csv\n",
      "1083_highgearmedia.com.csv\n",
      "1089_freep.com.csv\n",
      "1099_enthusiastnetwork.com.csv\n",
      "1106_allstate.com.csv\n",
      "1164_acbj.com.csv\n",
      "1205_opensecrets.org.csv\n",
      "1206_dcccd.edu.csv\n",
      "1221_gwdocs.com.csv\n",
      "1224_austincc.edu.csv\n",
      "1252_cincymuseum.org.csv\n",
      "1259_fool.com.csv\n",
      "1261_zacks.com.csv\n",
      "1264_citizen.org.csv\n",
      "1300_bankofamerica.com.csv\n",
      "1306_chasepaymentech.com.csv\n",
      "133_fortune.com.csv\n",
      "135_instagram.com.csv\n",
      "1360_thehill.com.csv\n",
      "1361_yahoo.com.csv\n",
      "1419_miaminewtimes.com.csv\n",
      "144_style.com.csv\n",
      "1468_rockstargames.com.csv\n",
      "1470_steampowered.com.csv\n",
      "1498_ticketmaster.com.csv\n",
      "1510_jibjab.com.csv\n",
      "1539_geocaching.com.csv\n",
      "1545_taylorswift.com.csv\n",
      "1582_msn.com.csv\n",
      "1610_post-gazette.com.csv\n",
      "1618_sltrib.com.csv\n",
      "1636_sidearmsports.com.csv\n",
      "1637_dailyillini.com.csv\n",
      "164_adweek.com.csv\n",
      "1666_wsmv.com.csv\n",
      "1673_tulsaworld.com.csv\n",
      "1683_dailynews.com.csv\n",
      "1694_lids.com.csv\n",
      "1703_sports-reference.com.csv\n",
      "1708_foxsports.com.csv\n",
      "1713_latinpost.com.csv\n",
      "175_mlb.mlb.com.csv\n",
      "186_abcnews.com.csv\n",
      "200_washingtonpost.com.csv\n",
      "202_foodallergy.org.csv\n",
      "207_reference.com.csv\n",
      "20_theatlantic.com.csv\n",
      "21_imdb.com.csv\n",
      "228_gawker.com.csv\n",
      "26_nytimes.com.csv\n",
      "303_reddit.com.csv\n",
      "320_timeinc.com.csv\n",
      "325_ocregister.com.csv\n",
      "32_voxmedia.com.csv\n",
      "331_tgifridays.com.csv\n",
      "33_nbcuniversal.com.csv\n",
      "348_walmart.com.csv\n",
      "359_vikings.com.csv\n",
      "394_newsbusters.org.csv\n",
      "414_washingtonian.com.csv\n",
      "453_barnesandnoble.com.csv\n",
      "456_boardgamegeek.com.csv\n",
      "481_fredericknewspost.com.csv\n",
      "503_buffalowildwings.com.csv\n",
      "517_kaleidahealth.org.csv\n",
      "523_usa.gov.csv\n",
      "531_archives.gov.csv\n",
      "541_ifsa-butler.org.csv\n",
      "559_www.loc.gov.csv\n",
      "571_abita.com.csv\n",
      "581_coffeereview.com.csv\n",
      "584_communitycoffee.com.csv\n",
      "586_cariboucoffee.com.csv\n",
      "58_esquire.com.csv\n",
      "591_google.com.csv\n",
      "59_liquor.com.csv\n",
      "627_dairyqueen.com.csv\n",
      "635_playstation.com.csv\n",
      "640_gamestop.com.csv\n",
      "641_cbsinteractive.com.csv\n",
      "642_thefreedictionary.com.csv\n",
      "652_randomhouse.com.csv\n",
      "676_restaurantnews.com.csv\n",
      "686_military.com.csv\n",
      "701_tangeroutlet.com.csv\n",
      "70_meredith.com.csv\n",
      "723_dogbreedinfo.com.csv\n",
      "744_minecraft.gamepedia.com.csv\n",
      "745_eatchicken.com.csv\n",
      "746_kraftrecipes.com.csv\n",
      "760_si.edu.csv\n",
      "777_education.jlab.org.csv\n",
      "807_lodgemfg.com.csv\n",
      "817_ironhorsevineyards.com.csv\n",
      "82_sheknows.com.csv\n",
      "856_sciencemag.org.csv\n",
      "862_disinfo.com.csv\n",
      "883_ted.com.csv\n",
      "884_naturalnews.com.csv\n",
      "891_everydayhealth.com.csv\n",
      "898_uptodate.com.csv\n",
      "907_earthkam.org.csv\n",
      "919_uh.edu.csv\n",
      "928_stlouisfed.org.csv\n",
      "93_pbs.org.csv\n",
      "940_internetbrands.com.csv\n",
      "962_lynda.com.csv\n",
      "988_solarviews.com.csv\n",
      "98_neworleansonline.com.csv\n",
      "995_mohegansun.com.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = r'annotations'  # use your path\n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "data_samples1 = []\n",
    "data_number1 = []\n",
    "data_label1 = []\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for file in allFiles:\n",
    "    allFiles = allFiles[:85]\n",
    "    dictCollection = {\"Action First-Party\":[],\n",
    "                      \"Purpose\":[],\n",
    "                      \"Personal Information Type\":[]\n",
    "                      }\n",
    "\n",
    "    df = pd.read_csv(file, thousands=',', header=None)\n",
    "    len(df)\n",
    "    # df_tail = df.tail(1)[4]\n",
    "    # print(df_tail)\n",
    "    number_of_segments = 5#len(df) + 1\n",
    "\n",
    "    file = file.split('annotations/', 1)[1]\n",
    "    print(file)\n",
    "    for i in range(number_of_segments-1):\n",
    "        # print(i)\n",
    "        if df[5][i] == \"First Party Collection/Use\":\n",
    "            parse_json = json.loads(str(df[6][i]))\n",
    "            if parse_json[\"Action First-Party\"][\"endIndexInSegment\"] != -1:\n",
    "                data_samples1.append(parse_json[\"Action First-Party\"][\"selectedText\"])\n",
    "                data_number1.append(1)\n",
    "                data_label1.append(\"Action First-Party\")\n",
    "                cnt = cnt+1\n",
    "            if parse_json[\"Personal Information Type\"][\"endIndexInSegment\"] != -1:\n",
    "                data_samples1.append(parse_json[\"Personal Information Type\"][\"selectedText\"])\n",
    "                data_number1.append(2)\n",
    "                data_label1.append(\"Other\")                \n",
    "                cnt = cnt+1\n",
    "            if parse_json[\"Purpose\"][\"endIndexInSegment\"] != -1:\n",
    "                data_samples1.append(parse_json[\"Purpose\"][\"selectedText\"])\n",
    "                data_number1.append(2)\n",
    "                data_label1.append(\"Other\")     \n",
    "                cnt = cnt+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 1 2 2 1 2 1 2 2 1 2 2 1 2 1 2 1 2 1 2 2 2 1 2 2 1 2 2 1 2 1 2 1 2 2\n",
      " 1 2 2 1 2 1 2 2 1 1 2 2 1 2 2 1 2 2 1 2 1 2 1 2 2 1 2 2 2 1 2 2 1 2 1 2 1\n",
      " 2 2 1 2 2 1 2 2 1 2 2 1 2 2 1 2 2 1 2 2 1 2 2 1 2 1 2 2 1 2 2 1 2 2 1 2 2\n",
      " 1 2 1 2 1 2 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_samples2 = np.array(data_samples1[:120])\n",
    "train_label2 = np.array(data_label1[:120])\n",
    "train_number2 = np.array(data_number1[:120])\n",
    "\n",
    "test_samples2 = np.array(data_samples1[121:167])\n",
    "test_label2 = np.array(data_label1[121:167])\n",
    "test_number2 = np.array(data_number1[121:167])\n",
    "print(train_number2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 144)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect1 = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=1000)\n",
    "X_train_counts1 = count_vect1.fit_transform(train_samples2)\n",
    "X_train_counts1\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer1 = TfidfTransformer(use_idf=False).fit(X_train_counts1)\n",
    "X_train_tf1 = tf_transformer1.transform(X_train_counts1)\n",
    "X_train_tf1.shape\n",
    "\n",
    "tfidf_transformer1 = TfidfTransformer()\n",
    "X_train_tfidf1 = tfidf_transformer1.fit_transform(X_train_counts1)\n",
    "X_train_tfidf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82608695652173914"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "text_clf1 = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "text_clf1 = text_clf1.fit(train_samples2, train_number2)\n",
    "\n",
    "import numpy as np\n",
    "predicted = text_clf1.predict(test_samples2)\n",
    "np.mean(predicted == test_number2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73913043478260865"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf1 = Pipeline([('vect', CountVectorizer()), \n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, n_iter=5, random_state=42))])\n",
    "_ = text_clf1.fit(train_samples2, train_number2)\n",
    "\n",
    "import numpy as np\n",
    "predicted = text_clf1.predict(test_samples2)\n",
    "np.mean(predicted == test_number2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1017_sci-news.com.csv\n",
      "1028_redorbit.com.csv\n",
      "1034_aol.com.csv\n",
      "1050_honda.com.csv\n",
      "105_amazon.com.csv\n",
      "1070_wnep.com.csv\n",
      "1083_highgearmedia.com.csv\n",
      "1089_freep.com.csv\n",
      "1099_enthusiastnetwork.com.csv\n",
      "1106_allstate.com.csv\n",
      "1164_acbj.com.csv\n",
      "1205_opensecrets.org.csv\n",
      "1206_dcccd.edu.csv\n",
      "1221_gwdocs.com.csv\n",
      "1224_austincc.edu.csv\n",
      "1252_cincymuseum.org.csv\n",
      "1259_fool.com.csv\n",
      "1261_zacks.com.csv\n",
      "1264_citizen.org.csv\n",
      "1300_bankofamerica.com.csv\n",
      "1306_chasepaymentech.com.csv\n",
      "133_fortune.com.csv\n",
      "135_instagram.com.csv\n",
      "1360_thehill.com.csv\n",
      "1361_yahoo.com.csv\n",
      "1419_miaminewtimes.com.csv\n",
      "144_style.com.csv\n",
      "1468_rockstargames.com.csv\n",
      "1470_steampowered.com.csv\n",
      "1498_ticketmaster.com.csv\n",
      "1510_jibjab.com.csv\n",
      "1539_geocaching.com.csv\n",
      "1545_taylorswift.com.csv\n",
      "1582_msn.com.csv\n",
      "1610_post-gazette.com.csv\n",
      "1618_sltrib.com.csv\n",
      "1636_sidearmsports.com.csv\n",
      "1637_dailyillini.com.csv\n",
      "164_adweek.com.csv\n",
      "1666_wsmv.com.csv\n",
      "1673_tulsaworld.com.csv\n",
      "1683_dailynews.com.csv\n",
      "1694_lids.com.csv\n",
      "1703_sports-reference.com.csv\n",
      "1708_foxsports.com.csv\n",
      "1713_latinpost.com.csv\n",
      "175_mlb.mlb.com.csv\n",
      "186_abcnews.com.csv\n",
      "200_washingtonpost.com.csv\n",
      "202_foodallergy.org.csv\n",
      "207_reference.com.csv\n",
      "20_theatlantic.com.csv\n",
      "21_imdb.com.csv\n",
      "228_gawker.com.csv\n",
      "26_nytimes.com.csv\n",
      "303_reddit.com.csv\n",
      "320_timeinc.com.csv\n",
      "325_ocregister.com.csv\n",
      "32_voxmedia.com.csv\n",
      "331_tgifridays.com.csv\n",
      "33_nbcuniversal.com.csv\n",
      "348_walmart.com.csv\n",
      "359_vikings.com.csv\n",
      "394_newsbusters.org.csv\n",
      "414_washingtonian.com.csv\n",
      "453_barnesandnoble.com.csv\n",
      "456_boardgamegeek.com.csv\n",
      "481_fredericknewspost.com.csv\n",
      "503_buffalowildwings.com.csv\n",
      "517_kaleidahealth.org.csv\n",
      "523_usa.gov.csv\n",
      "531_archives.gov.csv\n",
      "541_ifsa-butler.org.csv\n",
      "559_www.loc.gov.csv\n",
      "571_abita.com.csv\n",
      "581_coffeereview.com.csv\n",
      "584_communitycoffee.com.csv\n",
      "586_cariboucoffee.com.csv\n",
      "58_esquire.com.csv\n",
      "591_google.com.csv\n",
      "59_liquor.com.csv\n",
      "627_dairyqueen.com.csv\n",
      "635_playstation.com.csv\n",
      "640_gamestop.com.csv\n",
      "641_cbsinteractive.com.csv\n",
      "642_thefreedictionary.com.csv\n",
      "652_randomhouse.com.csv\n",
      "676_restaurantnews.com.csv\n",
      "686_military.com.csv\n",
      "701_tangeroutlet.com.csv\n",
      "70_meredith.com.csv\n",
      "723_dogbreedinfo.com.csv\n",
      "744_minecraft.gamepedia.com.csv\n",
      "745_eatchicken.com.csv\n",
      "746_kraftrecipes.com.csv\n",
      "760_si.edu.csv\n",
      "777_education.jlab.org.csv\n",
      "807_lodgemfg.com.csv\n",
      "817_ironhorsevineyards.com.csv\n",
      "82_sheknows.com.csv\n",
      "856_sciencemag.org.csv\n",
      "862_disinfo.com.csv\n",
      "883_ted.com.csv\n",
      "884_naturalnews.com.csv\n",
      "891_everydayhealth.com.csv\n",
      "898_uptodate.com.csv\n",
      "907_earthkam.org.csv\n",
      "919_uh.edu.csv\n",
      "928_stlouisfed.org.csv\n",
      "93_pbs.org.csv\n",
      "940_internetbrands.com.csv\n",
      "962_lynda.com.csv\n",
      "988_solarviews.com.csv\n",
      "98_neworleansonline.com.csv\n",
      "995_mohegansun.com.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = r'annotations'  # use your path\n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "data_samples3 = []\n",
    "data_number3 = []\n",
    "data_label3 = []\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for file in allFiles:\n",
    "    allFiles = allFiles[:85]\n",
    "    dictCollection = {\"Action First-Party\":[],\n",
    "                      \"Purpose\":[],\n",
    "                      \"Personal Information Type\":[]\n",
    "                      }\n",
    "\n",
    "    df = pd.read_csv(file, thousands=',', header=None)\n",
    "    len(df)\n",
    "    # df_tail = df.tail(1)[4]\n",
    "    # print(df_tail)\n",
    "    number_of_segments = 5#len(df) + 1\n",
    "\n",
    "    file = file.split('annotations/', 1)[1]\n",
    "    print(file)\n",
    "    for i in range(number_of_segments-1):\n",
    "        # print(i)\n",
    "        if df[5][i] == \"First Party Collection/Use\":\n",
    "            parse_json = json.loads(str(df[6][i]))\n",
    "            if parse_json[\"Action First-Party\"][\"endIndexInSegment\"] != -1:\n",
    "                data_samples3.append(parse_json[\"Action First-Party\"][\"selectedText\"])\n",
    "                data_number3.append(1)\n",
    "                data_label3.append(\"Other\")\n",
    "                cnt = cnt+1\n",
    "            if parse_json[\"Personal Information Type\"][\"endIndexInSegment\"] != -1:\n",
    "                data_samples3.append(parse_json[\"Personal Information Type\"][\"selectedText\"])\n",
    "                data_number3.append(1)\n",
    "                data_label3.append(\"Other\")                \n",
    "                cnt = cnt+1\n",
    "            if parse_json[\"Purpose\"][\"endIndexInSegment\"] != -1:\n",
    "                data_samples3.append(parse_json[\"Purpose\"][\"selectedText\"])\n",
    "                data_number3.append(2)\n",
    "                data_label3.append(\"Purpose\")     \n",
    "                cnt = cnt+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 1 1 2 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 2 1 1 1 1 1 1 2\n",
      " 1 1 2 1 1 1 1 2 1 1 1 2 1 1 2 1 1 2 1 1 1 1 1 1 2 1 1 2 1 1 1 2 1 1 1 1 1\n",
      " 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1 1 1 2 1 1 2 1 1 2 1 1 2\n",
      " 1 1 1 1 1 1 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "train_samples3 = np.array(data_samples3[:120])\n",
    "train_label3 = np.array(data_label3[:120])\n",
    "train_number3 = np.array(data_number3[:120])\n",
    "\n",
    "test_samples3 = np.array(data_samples3[121:167])\n",
    "test_label3 = np.array(data_label3[121:167])\n",
    "test_number3 = np.array(data_number3[121:167])\n",
    "print(train_number3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 144)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect1 = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=1000)\n",
    "X_train_counts1 = count_vect1.fit_transform(train_samples3)\n",
    "X_train_counts1\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer1 = TfidfTransformer(use_idf=False).fit(X_train_counts1)\n",
    "X_train_tf1 = tf_transformer1.transform(X_train_counts1)\n",
    "X_train_tf1.shape\n",
    "\n",
    "tfidf_transformer1 = TfidfTransformer()\n",
    "X_train_tfidf1 = tfidf_transformer1.fit_transform(X_train_counts1)\n",
    "X_train_tfidf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76086956521739135"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "text_clf1 = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "text_clf1 = text_clf1.fit(train_samples3, train_number3)\n",
    "\n",
    "import numpy as np\n",
    "predicted = text_clf1.predict(test_samples3)\n",
    "np.mean(predicted == test_number3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73913043478260865"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf1 = Pipeline([('vect', CountVectorizer()), \n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, n_iter=5, random_state=42))])\n",
    "_ = text_clf1.fit(train_samples3, train_number3)\n",
    "\n",
    "import numpy as np\n",
    "predicted = text_clf1.predict(test_samples3)\n",
    "np.mean(predicted == test_number3)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
