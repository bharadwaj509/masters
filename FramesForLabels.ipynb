{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of the health sentences from Personal Information Type\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "# the original code is in the SentenceClassification.ipytn file. I am eliminating many variables in this file.\n",
    "# This file is copied from the SentenceClassification_main file.\n",
    "# In here we are trying to extract the data related to the value \"Health\" in the category Personal Information Type\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import nltk.data\n",
    "\n",
    "path = r'Plain_Html'  # use your path\n",
    "allFiles = glob.glob(path + \"/*.html\")\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "path = r'Train'  # use your path\n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "\n",
    "\n",
    "allFiles = allFiles[:65]\n",
    "\n",
    "\n",
    "# all the followint positives are to be counted if the sentences are in that particular class. \n",
    "# A sen_neg to be counted in any other circumstances\n",
    "sen0_pos = 0 # action_first_party no. of sentence positives\n",
    "sen0_neg = 0 # action_first_party no. of sentence negatives\n",
    "sen_actioFP = []\n",
    "sen_actioFP_class = []\n",
    "\n",
    "sen1_pos = 0 # Personal Information Type no. of sentence positives\n",
    "sen1_neg = 0 # action_first_party no. of sentence negatives\n",
    "sen_personalIT = []\n",
    "sen_personalIT_class = []\n",
    "\n",
    "sen2_pos = 0 # purpose no. of sentence negatives\n",
    "sen2_neg = 0 # purpose no. of sentence negatives\n",
    "sen_purpose = []\n",
    "sen_purpose_class = []\n",
    "\n",
    "sen3_pos = 0\n",
    "\n",
    "def html_to_text(html):\n",
    "    \"Creates a formatted text email message as a string from a rendered html template (page)\"\n",
    "    html_report_part1 = open(html,'r')\n",
    "    soup = BeautifulSoup(html_report_part1, 'html.parser')\n",
    "    # Ignore anything in head tag\n",
    "    text = []\n",
    "    start_index = []\n",
    "    end_index = []\n",
    "    for element in soup.descendants:\n",
    "        # We use type and not isinstance since comments, cdata, etc are subclasses that we don't want\n",
    "        count = 0;\n",
    "        if type(element) == NavigableString:\n",
    "            # We use the assumption that other tags can't be inside a script or style\n",
    "            if element.parent.name in ('script', 'style'):\n",
    "                continue\n",
    "\n",
    "            # remove any multiple and leading/trailing whitespace\n",
    "            string = ' '.join(element.string.split())\n",
    "            if string:\n",
    "                if element.parent.name == 'a':\n",
    "                    a_tag = element.parent\n",
    "                    # replace link text with the link\n",
    "                    string = a_tag['href']\n",
    "                    # concatenate with any non-empty immediately previous string\n",
    "                    if (    type(a_tag.previous_sibling) == NavigableString and\n",
    "                            a_tag.previous_sibling.string.strip() ):\n",
    "                        text[-1] = text[-1] + ' ' + string\n",
    "                        continue\n",
    "                elif element.previous_sibling and element.previous_sibling.name == 'a':\n",
    "                    text[-1] = text[-1] + ' ' + string\n",
    "                    continue\n",
    "                elif element.parent.name == 'p':\n",
    "                    # Add extra paragraph formatting newline\n",
    "                    string = '\\n' + string\n",
    "                text += [string]\n",
    "                start_index.append(count)\n",
    "    doc = '\\n'.join(text)\n",
    "    return doc\n",
    "\n",
    "\n",
    "\n",
    "for file in allFiles:\n",
    "    \n",
    "    df = pd.read_csv(file, thousands=',', header=None)\n",
    "    len(df)\n",
    "    number_of_segments = len(df) + 1\n",
    "\n",
    "    file = file.split('Train/', 1)[1]\n",
    "\n",
    "    ### beautifulsuop on html doc\n",
    "    file_html = 'Plain_Html/'+file\n",
    "    file_html = file_html.split('csv', 1)[0]\n",
    "    file_html = file_html+'html'\n",
    "    ls = []\n",
    "    indeces = []\n",
    "    data = html_to_text(file_html)\n",
    "    ls = tokenizer.tokenize(data)\n",
    "    count = 0\n",
    "    for l in ls:\n",
    "        indeces.append([count,count+len(l)])\n",
    "        count = count+len(l)\n",
    "\n",
    "    for i in range(number_of_segments-1):\n",
    "        choice = 0 #whether the sentence is from one of the following categories\n",
    "        if df[5][i] == \"First Party Collection/Use\":\n",
    "            parse_json = json.loads(str(df[6][i]))\n",
    "            # parsing and fetching the text related to \"Action First Party\"            \n",
    "#             if parse_json[\"Action First-Party\"][\"endIndexInSegment\"] != -1:\n",
    "#                 choice = 1\n",
    "#                 str_index = parse_json[\"Action First-Party\"][\"startIndexInSegment\"]\n",
    "#                 end_index = parse_json[\"Action First-Party\"][\"endIndexInSegment\"]\n",
    "#                 for index in range(len(ls)):\n",
    "#                     parsed_sen = ls[index].replace(\"/\\n|||-\",\"\")                        \n",
    "#                     parsed_sen = ls[index].replace(\"|||\",\"\") \n",
    "#                     if((str_index > indeces[index][0] and end_index < indeces[index][1]) or \n",
    "#                        (str_index < indeces[index][0] and end_index > indeces[index][1]) or\n",
    "#                        (str_index < indeces[index][0] and end_index > indeces[index][0] and end_index < indeces[index][1]) or\n",
    "#                        (str_index > indeces[index][0] and str_index < indeces[index][1] and end_index > indeces[index][1])):                        \n",
    "#                         sen_actioFP.append(parsed_sen)\n",
    "# #                         sen_personalIT.append(ls[index])\n",
    "# #                         sen_purpose.append(ls[index])\n",
    "# #                         sen_actioFP_class.append(\"pos\")                        \n",
    "# #                         sen_personalIT_class.append(\"neg\")                        \n",
    "# #                         sen_purpose_class.append(\"neg\")\n",
    "#                         sen0_pos += 1\n",
    "#                         sen1_neg += 1\n",
    "#                         sen2_neg += 1\n",
    "# #                     else: \n",
    "# #                         sen_actioFP.append(parsed_sen)\n",
    "# #                         sen_personalIT.append(ls[index])\n",
    "# #                         sen_purpose.append(ls[index])\n",
    "# #                         sen_actioFP_class.append(\"neg\")                        \n",
    "# #                         sen_personalIT_class.append(\"neg\")                        \n",
    "# #                         sen_purpose_class.append(\"neg\")\n",
    "# #                         sen0_neg += 1\n",
    "# #                         sen1_neg += 1\n",
    "# #                         sen2_neg += 1\n",
    "            # parsing and fetching the text related to \"Personal Information Type\"                        \n",
    "            if parse_json[\"Personal Information Type\"][\"endIndexInSegment\"] != -1 and parse_json[\"Personal Information Type\"][\"value\"]==\"Health\":\n",
    "                choice = 1\n",
    "                str_index = parse_json[\"Personal Information Type\"][\"startIndexInSegment\"]\n",
    "                end_index = parse_json[\"Personal Information Type\"][\"endIndexInSegment\"]\n",
    "                \n",
    "                for index in range(len(ls)):\n",
    "                    parsed_sen = ls[index].replace(\"/\\n|||-\",\"\")                        \n",
    "                    parsed_sen = ls[index].replace(\"|||\",\"\") \n",
    "                    if((str_index > indeces[index][0] and end_index < indeces[index][1]) or \n",
    "                       (str_index < indeces[index][0] and end_index > indeces[index][1]) or\n",
    "                       (str_index < indeces[index][0] and end_index > indeces[index][0] and end_index < indeces[index][1]) or\n",
    "                       (str_index > indeces[index][0] and str_index < indeces[index][1] and end_index > indeces[index][1])):                        \n",
    "#                         sen_actioFP.append(parsed_sen)\n",
    "                        sen_personalIT.append([parsed_sen,parse_json[\"Personal Information Type\"][\"value\"]])\n",
    "#                         sen_purpose.append(ls[index])\n",
    "#                         sen_actioFP_class.append(\"neg\")                        \n",
    "#                         sen_personalIT_class.append(\"pos\")                        \n",
    "#                         sen_purpose_class.append(\"neg\")  \n",
    "                        sen0_neg += 1\n",
    "                        sen1_pos += 1\n",
    "                        sen2_neg += 1\n",
    "#                     else: \n",
    "#                         sen_actioFP.append(parsed_sen)\n",
    "#                         sen_personalIT.append(ls[index])\n",
    "#                         sen_purpose.append(ls[index])\n",
    "#                         sen_actioFP_class.append(\"neg\")                        \n",
    "#                         sen_personalIT_class.append(\"neg\")                        \n",
    "#                         sen_purpose_class.append(\"neg\")  \n",
    "#                         sen0_neg += 1\n",
    "#                         sen1_neg += 1\n",
    "#                         sen2_neg += 1\n",
    "            # parsing and fetching the text related to \"Purpose\"                \n",
    "#             if parse_json[\"Purpose\"][\"endIndexInSegment\"] != -1:\n",
    "#                 choice = 1            \n",
    "#                 str_index = parse_json[\"Purpose\"][\"startIndexInSegment\"]\n",
    "#                 end_index = parse_json[\"Purpose\"][\"endIndexInSegment\"]\n",
    "#                 for index in range(len(ls)):\n",
    "#                     parsed_sen = ls[index].replace(\"/\\n|||-\",\"\")                        \n",
    "#                     parsed_sen = ls[index].replace(\"|||\",\"\") \n",
    "#                     if((str_index > indeces[index][0] and end_index < indeces[index][1]) or \n",
    "#                        (str_index < indeces[index][0] and end_index > indeces[index][1]) or\n",
    "#                        (str_index < indeces[index][0] and end_index > indeces[index][0] and end_index < indeces[index][1]) or\n",
    "#                        (str_index > indeces[index][0] and str_index < indeces[index][1] and end_index > indeces[index][1])):\n",
    "# #                         sen_actioFP.append(parsed_sen)\n",
    "# #                         sen_personalIT.append(ls[index])\n",
    "#                         sen_purpose.append(parsed_sen)\n",
    "# #                         sen_actioFP_class.append(\"neg\")                        \n",
    "# #                         sen_personalIT_class.append(\"neg\")                        \n",
    "# #                         sen_purpose_class.append(\"pos\")    \n",
    "#                         sen0_neg += 1\n",
    "#                         sen1_neg += 1\n",
    "#                         sen2_pos += 1\n",
    "# #                     else: \n",
    "# #                         sen_actioFP.append(parsed_sen)\n",
    "# #                         sen_personalIT.append(ls[index])\n",
    "# #                         sen_purpose.append(ls[index])\n",
    "# #                         sen_actioFP_class.append(\"neg\")                        \n",
    "# #                         sen_personalIT_class.append(\"neg\")                        \n",
    "# #                         sen_purpose_class.append(\"neg\")                        \n",
    "# #                         sen0_neg += 1\n",
    "# #                         sen1_neg += 1\n",
    "# #                         sen2_neg += 1\n",
    "                                \n",
    "#         if df[5][i] == \"Third Party Sharing/Collection\":\n",
    "#             parse_json = json.loads(str(df[6][i]))\n",
    "#             if parse_json[\"Personal Information Type\"][\"endIndexInSegment\"] != -1:\n",
    "#                 choice = 1\n",
    "#                 str_index = parse_json[\"Personal Information Type\"][\"startIndexInSegment\"]\n",
    "#                 end_index = parse_json[\"Personal Information Type\"][\"endIndexInSegment\"]\n",
    "#                 for index in range(len(ls)):                    \n",
    "#                     if((str_index > indeces[index][0] and end_index < indeces[index][1]) or \n",
    "#                        (str_index < indeces[index][0] and end_index > indeces[index][1]) or\n",
    "#                        (str_index < indeces[index][0] and end_index > indeces[index][0] and end_index < indeces[index][1]) or\n",
    "#                        (str_index > indeces[index][0] and str_index < indeces[index][1] and end_index > indeces[index][1])):\n",
    "#                         sen0_neg += 1\n",
    "#                         sen1_pos += 1\n",
    "#                         sen2_neg += 1\n",
    "#                     else: \n",
    "#                         sen0_neg += 1\n",
    "#                         sen1_neg += 1                        \n",
    "#                         sen2_neg += 1\n",
    "\n",
    "#             if parse_json[\"Purpose\"][\"endIndexInSegment\"] != -1:\n",
    "#                 choice = 1\n",
    "#                 total_data_samples2.append(parse_json[\"Purpose\"][\"selectedText\"])\n",
    "#                 total_data_number2.append(2)\n",
    "#                 total_data_label2.append(\"Purpose\") \n",
    "#                 str_index = parse_json[\"Purpose\"][\"startIndexInSegment\"]\n",
    "#                 end_index = parse_json[\"Purpose\"][\"endIndexInSegment\"]\n",
    "#                 for index in range(len(ls)):\n",
    "#                     if((str_index > indeces[index][0] and end_index < indeces[index][1]) or \n",
    "#                        (str_index < indeces[index][0] and end_index > indeces[index][1]) or\n",
    "#                        (str_index < indeces[index][0] and end_index > indeces[index][0] and end_index < indeces[index][1]) or\n",
    "#                        (str_index > indeces[index][0] and str_index < indeces[index][1] and end_index > indeces[index][1])):\n",
    "#                         sen0_neg += 1\n",
    "#                         sen1_neg += 1\n",
    "#                         sen2_pos += 1\n",
    "#                     else: \n",
    "#                         sen0_neg += 1\n",
    "#                         sen1_neg += 1                        \n",
    "#                         sen2_neg += 1\n",
    "\n",
    "#         if df[5][i] == \"User Choice/Control\":\n",
    "#             parse_json = json.loads(str(df[6][i]))\n",
    "#             if parse_json[\"Personal Information Type\"][\"endIndexInSegment\"] != -1:\n",
    "#                 choice = 1\n",
    "#                 str_index = parse_json[\"Personal Information Type\"][\"startIndexInSegment\"]\n",
    "#                 end_index = parse_json[\"Personal Information Type\"][\"endIndexInSegment\"]\n",
    "#                 for index in range(len(ls)):\n",
    "#                     if((str_index > indeces[index][0] and end_index < indeces[index][1]) or \n",
    "#                        (str_index < indeces[index][0] and end_index > indeces[index][1]) or\n",
    "#                        (str_index < indeces[index][0] and end_index > indeces[index][0] and end_index < indeces[index][1]) or\n",
    "#                        (str_index > indeces[index][0] and str_index < indeces[index][1] and end_index > indeces[index][1])):\n",
    "#                         sen0_neg += 1\n",
    "#                         sen1_pos += 1\n",
    "#                         sen2_neg += 1\n",
    "#                     else: \n",
    "#                         sen0_neg += 1\n",
    "#                         sen1_neg += 1                        \n",
    "#                         sen2_neg += 1\n",
    "            \n",
    "#             if parse_json[\"Purpose\"][\"endIndexInSegment\"] != -1:\n",
    "#                 choice = 1\n",
    "#                 str_index = parse_json[\"Purpose\"][\"startIndexInSegment\"]\n",
    "#                 end_index = parse_json[\"Purpose\"][\"endIndexInSegment\"]\n",
    "#                 for index in range(len(ls)):\n",
    "#                     if((str_index > indeces[index][0] and end_index < indeces[index][1]) or \n",
    "#                        (str_index < indeces[index][0] and end_index > indeces[index][1]) or\n",
    "#                        (str_index < indeces[index][0] and end_index > indeces[index][0] and end_index < indeces[index][1]) or\n",
    "#                        (str_index > indeces[index][0] and str_index < indeces[index][1] and end_index > indeces[index][1])):\n",
    "#                         sen0_neg += 1\n",
    "#                         sen1_neg += 1\n",
    "#                         sen2_pos += 1\n",
    "#                     else: \n",
    "#                         sen0_neg += 1\n",
    "#                         sen1_neg += 1                        \n",
    "#                         sen2_neg += 1\n",
    "\n",
    "#         if df[5][i] == \"Data Retention\":\n",
    "#             parse_json = json.loads(str(df[6][i]))\n",
    "#             if parse_json[\"Personal Information Type\"][\"endIndexInSegment\"] != -1:\n",
    "#                 choice = 1\n",
    "#                 str_index = parse_json[\"Personal Information Type\"][\"startIndexInSegment\"]\n",
    "#                 end_index = parse_json[\"Personal Information Type\"][\"endIndexInSegment\"]\n",
    "#                 for index in range(len(ls)):\n",
    "#                     if((str_index > indeces[index][0] and end_index < indeces[index][1]) or \n",
    "#                        (str_index < indeces[index][0] and end_index > indeces[index][1]) or\n",
    "#                        (str_index < indeces[index][0] and end_index > indeces[index][0] and end_index < indeces[index][1]) or\n",
    "#                        (str_index > indeces[index][0] and str_index < indeces[index][1] and end_index > indeces[index][1])):\n",
    "#                         sen0_neg += 1\n",
    "#                         sen1_pos += 1                        \n",
    "#                         sen2_neg += 1\n",
    "#                     else: \n",
    "#                         sen0_neg += 1\n",
    "#                         sen1_neg += 1                        \n",
    "#                         sen2_neg += 1\n",
    "                            \n",
    "                            \n",
    "# print(\"----- postive and -ve sentences\")                \n",
    "# print(len(sen_actioFP),len(sen_actioFP_class))\n",
    "# print(len(sen_personalIT),len(sen_personalIT_class))\n",
    "# print(len(sen_purpose),len(sen_purpose_class))\n",
    "# print(len(sentence_list))\n",
    "\n",
    "\n",
    "# print(sen0_pos, sen0_neg)\n",
    "# print(sen1_pos, sen1_neg)\n",
    "# print(sen2_pos, sen2_neg)\n",
    "\n",
    "\n",
    "print(\"len of the health sentences from Personal Information Type\")\n",
    "print(len(sen_personalIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Privacy Policy\\nSci-News.com is committed to protecting and respecting your privacy.', 'To better inform you of our policy concerning user privacy, we have adopted the following terms.', 'Privacy Policy\\nSci-News.com is committed to protecting and respecting your privacy.', 'To better inform you of our policy concerning user privacy, we have adopted the following terms.', 'Privacy Policy\\nSci-News.com is committed to protecting and respecting your privacy.', 'Privacy Policy\\nSci-News.com is committed to protecting and respecting your privacy.', 'Privacy Policy\\nSci-News.com is committed to protecting and respecting your privacy.', 'Please note that these terms are subject to change, and any such changes will be included on this page.', 'Please note that these terms are subject to change, and any such changes will be included on this page.', 'Please note that these terms are subject to change, and any such changes will be included on this page.']\n",
      "4464\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(sen_actioFP[:10])\n",
    "print(len(sen_actioFP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Information that Sci-News.com May Collect Online\\nSci-News.com may collect and process the following data about you:\\n- information that you provide by filling in forms on our site, including names, e-mail and website addresses; we may also ask you for information for other purposes, for example when you report a problem with our site;\\n- if you contact us, we may keep a record of that correspondence;\\n- details of your visits to our site including, but not limited to, traffic data, location data, weblogs and other communication data.', 'Information that Sci-News.com May Collect Online\\nSci-News.com may collect and process the following data about you:\\n- information that you provide by filling in forms on our site, including names, e-mail and website addresses; we may also ask you for information for other purposes, for example when you report a problem with our site;\\n- if you contact us, we may keep a record of that correspondence;\\n- details of your visits to our site including, but not limited to, traffic data, location data, weblogs and other communication data.', 'Information that Sci-News.com May Collect Online\\nSci-News.com may collect and process the following data about you:\\n- information that you provide by filling in forms on our site, including names, e-mail and website addresses; we may also ask you for information for other purposes, for example when you report a problem with our site;\\n- if you contact us, we may keep a record of that correspondence;\\n- details of your visits to our site including, but not limited to, traffic data, location data, weblogs and other communication data.', 'To better inform you of our policy concerning user privacy, we have adopted the following terms.', 'Please note that these terms are subject to change, and any such changes will be included on this page.', 'To better inform you of our policy concerning user privacy, we have adopted the following terms.', 'Please note that these terms are subject to change, and any such changes will be included on this page.', 'Information that Sci-News.com May Collect Online\\nSci-News.com may collect and process the following data about you:\\n- information that you provide by filling in forms on our site, including names, e-mail and website addresses; we may also ask you for information for other purposes, for example when you report a problem with our site;\\n- if you contact us, we may keep a record of that correspondence;\\n- details of your visits to our site including, but not limited to, traffic data, location data, weblogs and other communication data.', 'To better inform you of our policy concerning user privacy, we have adopted the following terms.', 'Please note that these terms are subject to change, and any such changes will be included on this page.']\n"
     ]
    }
   ],
   "source": [
    "print(sen_purpose[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extracted texts for each and every attribute\n",
    "\n",
    "\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "\n",
    "# To prevent butter from over-browning in your pan, add a little bit of lemon juice.\n",
    "\n",
    "# lines = []\n",
    "# while True:\n",
    "#     line = input()\n",
    "#     if line:\n",
    "#         lines.append(line)\n",
    "#     else:\n",
    "#         break\n",
    "text = '\\n'.join(sen_actioFP[:200])\n",
    "\n",
    "text_file = open(\"data/aa.txt\", \"w\")\n",
    "text_file.write(\"%s\" % text)\n",
    "text_file.close()\n",
    "\n",
    "text_personalIT = '\\n'.join(sen_personalIT[:200])\n",
    "\n",
    "text_file = open(\"data/bb.txt\", \"w\")\n",
    "text_file.write(\"%s\" % text_personalIT)\n",
    "text_file.close()\n",
    "\n",
    "text_purpose = '\\n'.join(sen_purpose[:200])\n",
    "\n",
    "text_file = open(\"data/cc.txt\", \"w\")\n",
    "text_file.write(\"%s\" % text_purpose)\n",
    "text_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the frames and frame elements for 'action first party'\n",
    "subprocess.call(['/Users/aryan/Desktop/personal_projects/semafor3/semafor/bin/runMalt.sh',\n",
    "                '/Users/aryan/Desktop/masters/MachineLearningModels/data/actioFP.txt',\n",
    "               '/Users/aryan/Desktop/masters/MachineLearningModels/data/'])\n",
    "\n",
    "os.system('cat /Users/aryan/Desktop/masters/MachineLearningModels/data/conll | nc localhost 8081 > /Users/aryan/Desktop/masters/MachineLearningModels/data/output.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Privacy Policy\n",
      "Sci-News.com is committed to protecting and respecting your privacy.\n",
      "Frame is -->  Stimulus_focus\n",
      "Text is -->  pleasant\n",
      "Frame Element Name --> Stimulus\n",
      "Frame Element Text --> morning\n",
      "-------\n",
      "Frame is -->  Calendric_unit\n",
      "Text is -->  morning\n",
      "-------\n",
      "-----------\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "count = 0\n",
    "with open('/Users/aryan/Desktop/personal_projects/semafor3/data/output.json') as f:\n",
    "    for line in f:\n",
    "        print(sen_actioFP[count])\n",
    "        data = json.loads(line)\n",
    "        # ss = [data[i]['frames'][0]['annotationSets'][0]['frameElements'][0]['name'] for i in range(len(data))]\n",
    "        frames1 = []\n",
    "        frame_elements1 = []\n",
    "#         print(data.get('frames'))\n",
    "        for i in range(len(data.get('frames'))):\n",
    "            print(\"Frame is --> \",data.get('frames')[i].get('target').get('name'))\n",
    "            \n",
    "            for j in range(len(data.get('frames')[i].get('target').get('spans'))):\n",
    "                print(\"Text is --> \",data.get('frames')[i].get('target').get('spans')[j].get('text'))\n",
    "                \n",
    "            for k in range(len(data.get('frames')[i].get('annotationSets'))):\n",
    "                for l in range(len(data.get('frames')[i].get('annotationSets')[k].get('frameElements'))):\n",
    "                    print(\"Frame Element Name -->\",data.get('frames')[i].get('annotationSets')[k].get('frameElements')[l].get('name'))\n",
    "                    for m in range(len(data.get('frames')[i].get('annotationSets')[k].get('frameElements')[l].get('spans'))):\n",
    "                        print(\"Frame Element Text -->\",data.get('frames')[i].get('annotationSets')[k].get('frameElements')[l].get('spans')[m].get('text'))\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "            print(\"-------\")\n",
    "        count+=1\n",
    "        print(\"-----------\")\n",
    "        print(\"-----------\")\n",
    "#             frames1.append(data['frames'][j]['target']['name'])\n",
    "#             for k in range(len(data[i]['frames'][j]['annotationSets'][0]['frameElements'])):\n",
    "#                 frame_elements1.append(data[i]['frames'][j]['annotationSets'][0]['frameElements'][k]['name'])\n",
    "#         print(frames1)\n",
    "#         print(frame_elements1)\n",
    "#     print(data[i]['frames'])\n",
    "\n",
    "# print(set(frames))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
