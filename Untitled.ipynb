{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain_Html/1083_highgearmedia.com.html\n",
      "86\n",
      "86\n",
      "120 143\n",
      "120 143\n",
      "145 152\n",
      "145 152\n",
      "387 416\n",
      "62 87\n",
      "151 157\n",
      "151 157\n",
      "115 141\n",
      "145 152\n",
      "Plain_Html/1099_enthusiastnetwork.com.html\n",
      "159\n",
      "159\n",
      "33 73\n",
      "35 55\n",
      "131 151\n",
      "131 151\n",
      "76 137\n",
      "362 398\n",
      "28 59\n",
      "573 579\n",
      "630 637\n",
      "44 72\n",
      "35 55\n",
      "126 151\n",
      "250 265\n",
      "384 399\n",
      "171 178\n",
      "35 55\n",
      "121 137\n",
      "Plain_Html/1224_austincc.edu.html\n",
      "37\n",
      "37\n",
      "144 170\n",
      "104 121\n",
      "104 121\n",
      "Plain_Html/1470_steampowered.com.html\n",
      "70\n",
      "70\n",
      "27 34\n",
      "27 34\n",
      "0 70\n",
      "77 97\n",
      "25 97\n",
      "Plain_Html/1498_ticketmaster.com.html\n",
      "143\n",
      "143\n",
      "113 119\n",
      "113 119\n",
      "0 43\n",
      "80 91\n",
      "95 128\n",
      "16 42\n",
      "80 91\n",
      "46 57\n",
      "Plain_Html/1582_msn.com.html\n",
      "73\n",
      "73\n",
      "460 467\n",
      "19 43\n",
      "103 108\n",
      "296 333\n",
      "7 30\n",
      "460 467\n",
      "189 206\n",
      "103 108\n",
      "296 334\n",
      "Plain_Html/1637_dailyillini.com.html\n",
      "25\n",
      "25\n",
      "26 61\n",
      "Plain_Html/202_foodallergy.org.html\n",
      "54\n",
      "54\n",
      "95 105\n",
      "95 105\n",
      "20 71\n",
      "112 132\n",
      "152 166\n",
      "237 250\n",
      "20 71\n",
      "29 84\n",
      "152 166\n",
      "344 350\n",
      "244 250\n",
      "20 71\n",
      "29 84\n",
      "Plain_Html/228_gawker.com.html\n",
      "86\n",
      "86\n",
      "123 130\n",
      "123 130\n",
      "123 130\n",
      "123 130\n",
      "67 74\n",
      "67 74\n",
      "67 74\n",
      "101 108\n",
      "101 108\n",
      "101 108\n",
      "Plain_Html/303_reddit.com.html\n",
      "137\n",
      "137\n",
      "19 26\n",
      "97 123\n",
      "184 208\n",
      "Plain_Html/394_newsbusters.org.html\n",
      "121\n",
      "121\n",
      "158 165\n",
      "139 165\n",
      "206 214\n",
      "147 182\n",
      "147 182\n",
      "147 182\n",
      "Plain_Html/414_washingtonian.com.html\n",
      "73\n",
      "73\n",
      "127 147\n",
      "204 250\n",
      "440 447\n",
      "125 147\n",
      "212 218\n",
      "35 55\n",
      "315 322\n",
      "127 147\n",
      "Plain_Html/453_barnesandnoble.com.html\n",
      "273\n",
      "273\n",
      "277 307\n",
      "277 307\n",
      "277 307\n",
      "230 274\n",
      "220 228\n",
      "193 200\n",
      "133 139\n",
      "54 74\n",
      "54 74\n",
      "81 122\n",
      "81 122\n",
      "158 165\n",
      "158 165\n",
      "135 147\n",
      "135 147\n",
      "158 165\n",
      "158 165\n",
      "158 165\n",
      "158 165\n",
      "158 165\n",
      "101 121\n",
      "101 121\n",
      "403 422\n",
      "403 422\n",
      "Plain_Html/523_usa.gov.html\n",
      "47\n",
      "47\n",
      "255 331\n",
      "83 103\n",
      "44 62\n",
      "207 214\n",
      "168 188\n",
      "433 447\n",
      "Plain_Html/652_randomhouse.com.html\n",
      "116\n",
      "116\n",
      "109 125\n",
      "665 690\n",
      "139 176\n",
      "80 250\n",
      "126 132\n",
      "126 132\n",
      "160 173\n",
      "160 173\n",
      "160 173\n",
      "44 57\n",
      "230 250\n",
      "230 250\n",
      "357 371\n",
      "357 371\n",
      "Plain_Html/744_minecraft.gamepedia.com.html\n",
      "86\n",
      "86\n",
      "53 73\n",
      "249 270\n",
      "76 155\n",
      "76 155\n",
      "Plain_Html/82_sheknows.com.html\n",
      "131\n",
      "131\n",
      "37 47\n",
      "112 147\n",
      "112 135\n",
      "112 147\n",
      "0 346\n",
      "Plain_Html/856_sciencemag.org.html\n",
      "116\n",
      "116\n",
      "189 259\n",
      "189 259\n",
      "28 57\n",
      "824 838\n",
      "124 135\n",
      "124 135\n",
      "71 78\n",
      "97 117\n",
      "208 245\n",
      "208 245\n",
      "158 179\n",
      "198 206\n",
      "252 278\n",
      "368 375\n",
      "410 416\n",
      "102 119\n",
      "198 204\n",
      "39 66\n",
      "824 839\n",
      "124 135\n",
      "71 78\n",
      "71 78\n",
      "71 78\n",
      "71 78\n",
      "71 78\n",
      "172 179\n",
      "198 204\n",
      "198 204\n",
      "368 376\n",
      "272 278\n",
      "410 416\n",
      "198 204\n",
      "45 58\n",
      "72 128\n",
      "50 58\n",
      "Plain_Html/940_internetbrands.com.html\n",
      "187\n",
      "187\n",
      "384 404\n",
      "384 404\n",
      "384 404\n",
      "51 71\n",
      "141 161\n",
      "34 45\n",
      "34 45\n",
      "196 208\n",
      "275 302\n",
      "275 302\n",
      "275 302\n",
      "379 404\n",
      "379 404\n",
      "379 404\n",
      "23 45\n",
      "23 45\n",
      "15 37\n",
      "251 265\n",
      "251 265\n",
      "251 265\n",
      "251 265\n",
      "715 731\n",
      "296 310\n",
      "296 310\n",
      "296 310\n",
      "218 246\n",
      "384 404\n",
      "384 404\n",
      "384 404\n",
      "350 361\n",
      "562 581\n",
      "141 161\n",
      "34 45\n",
      "34 45\n",
      "196 208\n",
      "251 265\n",
      "251 265\n",
      "750 775\n",
      "781 810\n",
      "251 265\n",
      "251 265\n",
      "723 731\n",
      "723 731\n",
      "296 310\n",
      "296 310\n",
      "296 310\n",
      "641 675\n",
      "681 741\n",
      "631 667\n",
      "631 667\n",
      "Plain_Html/98_neworleansonline.com.html\n",
      "61\n",
      "61\n",
      "442 453\n",
      "19 148\n",
      "116 147\n",
      "116 147\n",
      "66 79\n",
      "182 199\n",
      "66 85\n",
      "92 133\n",
      "9 33\n",
      "1368 151513\n",
      "1780 204197\n",
      "1759 164304\n",
      "---\n",
      "1208\n",
      "2726\n",
      "2354\n",
      "1574\n",
      "--\n",
      "7862\n",
      "1208\n",
      "2726\n",
      "2354\n",
      "1574\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import nltk.data\n",
    "\n",
    "path = r'Plain_Html'  # use your path\n",
    "allFiles = glob.glob(path + \"/*.html\")\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "path = r'Test'  # use your path\n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "#action first party\n",
    "total_data_samples0 = []\n",
    "total_data_number0 = []\n",
    "total_data_label0 = []\n",
    "\n",
    "#Personal Info Type\n",
    "total_data_samples1 = []\n",
    "total_data_number1 = []\n",
    "total_data_label1 = []\n",
    "\n",
    "\n",
    "#Purpose\n",
    "total_data_samples2 = []\n",
    "total_data_number2 = []\n",
    "total_data_label2 = []\n",
    "\n",
    "\n",
    "#None\n",
    "total_data_samples3 = []\n",
    "total_data_number3 = []\n",
    "total_data_label3 = []\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "cnt0 = 0\n",
    "cnt1 = 0\n",
    "cnt2 = 0\n",
    "cnt3 = 0\n",
    "choice = 0\n",
    "allFiles = allFiles[:65]\n",
    "\n",
    "sen0_pos = 0 # action_first_party no. of sentence positives\n",
    "sen0_neg = 0 # action_first_party no. of sentence negatives\n",
    "\n",
    "sen1_pos = 0 # Personal Information Type no. of sentence positives\n",
    "sen1_neg = 0 # action_first_party no. of sentence negatives\n",
    "\n",
    "sen2_pos = 0 # purpose no. of sentence negatives\n",
    "sen2_neg = 0 # purpose no. of sentence negatives\n",
    "\n",
    "\n",
    "def html_to_text(html):\n",
    "    \"Creates a formatted text email message as a string from a rendered html template (page)\"\n",
    "    html_report_part1 = open(html,'r')\n",
    "    soup = BeautifulSoup(html_report_part1, 'html.parser')\n",
    "    # Ignore anything in head\n",
    "#     print(len(list(soup.descendants)))\n",
    "    text = []\n",
    "    start_index = []\n",
    "    end_index = []\n",
    "    for element in soup.descendants:\n",
    "#         print(element)\n",
    "        # We use type and not isinstance since comments, cdata, etc are subclasses that we don't want\n",
    "        count = 0;\n",
    "        if type(element) == NavigableString:\n",
    "            # We use the assumption that other tags can't be inside a script or style\n",
    "            if element.parent.name in ('script', 'style'):\n",
    "                continue\n",
    "\n",
    "            # remove any multiple and leading/trailing whitespace\n",
    "            string = ' '.join(element.string.split())\n",
    "            if string:\n",
    "                if element.parent.name == 'a':\n",
    "                    a_tag = element.parent\n",
    "                    # replace link text with the link\n",
    "                    string = a_tag['href']\n",
    "                    # concatenate with any non-empty immediately previous string\n",
    "                    if (    type(a_tag.previous_sibling) == NavigableString and\n",
    "                            a_tag.previous_sibling.string.strip() ):\n",
    "                        text[-1] = text[-1] + ' ' + string\n",
    "                        continue\n",
    "                elif element.previous_sibling and element.previous_sibling.name == 'a':\n",
    "                    text[-1] = text[-1] + ' ' + string\n",
    "                    continue\n",
    "                elif element.parent.name == 'p':\n",
    "                    # Add extra paragraph formatting newline\n",
    "                    string = '\\n' + string\n",
    "                text += [string]\n",
    "#                 print(len(string))\n",
    "                start_index.append(count)\n",
    "#                 print(count)\n",
    "    doc = '\\n'.join(text)\n",
    "    return doc\n",
    "\n",
    "sentence_list = []\n",
    "\n",
    "for file in allFiles:\n",
    "    dictCollection = {\"Action First-Party\":[],\n",
    "                      \"Purpose\":[],\n",
    "                      \"Personal Information Type\":[],\n",
    "                      \"None\":[]\n",
    "                      }\n",
    "\n",
    "    df = pd.read_csv(file, thousands=',', header=None)\n",
    "    len(df)\n",
    "    # df_tail = df.tail(1)[4]\n",
    "    # print(df_tail)\n",
    "    number_of_segments = len(df) + 1\n",
    "\n",
    "    file = file.split('Test/', 1)[1]\n",
    "#     print(file)\n",
    "\n",
    "    ### beautifulsuop on html doc\n",
    "    file_html = 'Plain_Html/'+file\n",
    "    file_html = file_html.split('csv', 1)[0]\n",
    "    file_html = file_html+'html'\n",
    "    print(file_html)\n",
    "    ls = []\n",
    "    indeces = []\n",
    "    data = html_to_text(file_html)\n",
    "    ls = tokenizer.tokenize(data)\n",
    "    count = 0\n",
    "    for l in ls:\n",
    "        indeces.append([count,count+len(l)])\n",
    "        count = count+len(l)\n",
    "    print(len(indeces))\n",
    "    print(len(ls))\n",
    "    ###\n",
    "    \n",
    "    \n",
    "    for i in range(number_of_segments-1):\n",
    "#         print(i)\n",
    "        choice = 0 #whether the sentence is from one of the following categories\n",
    "        if df[5][i] == \"First Party Collection/Use\":\n",
    "            choice = 1\n",
    "            parse_json = json.loads(str(df[6][i]))\n",
    "#             print(parse_json)\n",
    "            if parse_json[\"Action First-Party\"][\"endIndexInSegment\"] != -1:\n",
    "#                 print(\"ActionFirst Party\",parse_json[\"Action First-Party\"][\"startIndexInSegment\"],parse_json[\"Action First-Party\"][\"endIndexInSegment\"])\n",
    "                total_data_samples0.append(parse_json[\"Action First-Party\"][\"selectedText\"])\n",
    "                total_data_number0.append(0)\n",
    "                total_data_label0.append(\"Action First-Party\")\n",
    "                str_index = parse_json[\"Action First-Party\"][\"startIndexInSegment\"]\n",
    "                end_index = parse_json[\"Action First-Party\"][\"endIndexInSegment\"]\n",
    "                for index in range(len(ls)):\n",
    "                    if((str_index > indeces[index][0] and end_index < indeces[index][1]) or \n",
    "                       (str_index < indeces[index][0] and end_index > indeces[index][1]) or\n",
    "                       (str_index < indeces[index][0] and end_index > indeces[index][0] and end_index < indeces[index][1]) or\n",
    "                       (str_index > indeces[index][0] and str_index < indeces[index][1] and end_index > indeces[index][1])):\n",
    "                        sentence_list.append([ls[index],\"Action First-Party\",\"positive\"])\n",
    "                        sen0_pos += 1\n",
    "                    else: \n",
    "                        sentence_list.append([ls[index],\"Action First-Party\",\"negative\"])\n",
    "                        sen0_neg += 1\n",
    "                cnt0 = cnt0+1\n",
    "            if parse_json[\"Personal Information Type\"][\"endIndexInSegment\"] != -1:\n",
    "#                 print(\"Personal Information Type\",parse_json[\"Personal Information Type\"][\"startIndexInSegment\"],parse_json[\"Personal Information Type\"][\"endIndexInSegment\"])\n",
    "                total_data_samples1.append(parse_json[\"Personal Information Type\"][\"selectedText\"])\n",
    "                total_data_number1.append(1)\n",
    "                total_data_label1.append(\"Personal Information Type\")                \n",
    "                str_index = parse_json[\"Personal Information Type\"][\"startIndexInSegment\"]\n",
    "                end_index = parse_json[\"Personal Information Type\"][\"endIndexInSegment\"]\n",
    "                for index in range(len(ls)):\n",
    "                    if((str_index > indeces[index][0] and end_index < indeces[index][1]) or \n",
    "                       (str_index < indeces[index][0] and end_index > indeces[index][1]) or\n",
    "                       (str_index < indeces[index][0] and end_index > indeces[index][0] and end_index < indeces[index][1]) or\n",
    "                       (str_index > indeces[index][0] and str_index < indeces[index][1] and end_index > indeces[index][1])):\n",
    "                        sentence_list.append([ls[index],\"Personal Information Type\",\"positive\"])\n",
    "                        sen1_pos += 1\n",
    "                    else: \n",
    "                        sentence_list.append([ls[index],\"Personal Information Type\",\"negative\"])\n",
    "                        sen1_neg += 1\n",
    "                cnt1 = cnt1+1\n",
    "            if parse_json[\"Purpose\"][\"endIndexInSegment\"] != -1:\n",
    "#                 print(\"Purpose\",parse_json[\"Purpose\"][\"startIndexInSegment\"],parse_json[\"Purpose\"][\"endIndexInSegment\"])\n",
    "                total_data_samples2.append(parse_json[\"Purpose\"][\"selectedText\"])\n",
    "                total_data_number2.append(2)\n",
    "                total_data_label2.append(\"Purpose\")     \n",
    "                str_index = parse_json[\"Purpose\"][\"startIndexInSegment\"]\n",
    "                end_index = parse_json[\"Purpose\"][\"endIndexInSegment\"]\n",
    "                for index in range(len(ls)):\n",
    "                    if((str_index > indeces[index][0] and end_index < indeces[index][1]) or \n",
    "                       (str_index < indeces[index][0] and end_index > indeces[index][1]) or\n",
    "                       (str_index < indeces[index][0] and end_index > indeces[index][0] and end_index < indeces[index][1]) or\n",
    "                       (str_index > indeces[index][0] and str_index < indeces[index][1] and end_index > indeces[index][1])):\n",
    "                        sentence_list.append([ls[index],\"Purpose\",\"positive\"])\n",
    "                        sen2_pos += 1\n",
    "                    else: \n",
    "                        sentence_list.append([ls[index],\"Purpose\",\"negative\"])\n",
    "                        sen2_neg += 1\n",
    "                cnt2 = cnt2+1\n",
    "        if df[5][i] == \"Third Party Sharing/Collection\":\n",
    "            choice = 1\n",
    "            parse_json = json.loads(str(df[6][i]))\n",
    "#             print(parse_json)\n",
    "            if parse_json[\"Personal Information Type\"][\"endIndexInSegment\"] != -1:\n",
    "                total_data_samples1.append(parse_json[\"Personal Information Type\"][\"selectedText\"])\n",
    "                total_data_number1.append(1)\n",
    "                total_data_label1.append(\"Personal Information Type\")                \n",
    "                cnt1 = cnt1+1\n",
    "            if parse_json[\"Purpose\"][\"endIndexInSegment\"] != -1:\n",
    "                total_data_samples2.append(parse_json[\"Purpose\"][\"selectedText\"])\n",
    "                total_data_number2.append(2)\n",
    "                total_data_label2.append(\"Purpose\")     \n",
    "                cnt2 = cnt2+1\n",
    "        if df[5][i] == \"User Choice/Control\":\n",
    "            choice = 1\n",
    "            parse_json = json.loads(str(df[6][i]))\n",
    "            if parse_json[\"Personal Information Type\"][\"endIndexInSegment\"] != -1:\n",
    "                total_data_samples1.append(parse_json[\"Personal Information Type\"][\"selectedText\"])\n",
    "                print(parse_json[\"Personal Information Type\"][\"startIndexInSegment\"],parse_json[\"Personal Information Type\"][\"endIndexInSegment\"])\n",
    "                total_data_number1.append(1)\n",
    "                total_data_label1.append(\"Personal Information Type\")                \n",
    "                cnt1 = cnt1+1\n",
    "            if parse_json[\"Purpose\"][\"endIndexInSegment\"] != -1:\n",
    "                total_data_samples2.append(parse_json[\"Purpose\"][\"selectedText\"])\n",
    "                total_data_number2.append(2)\n",
    "                total_data_label2.append(\"Purpose\")     \n",
    "                cnt2 = cnt2+1\n",
    "        if df[5][i] == \"Data Retention\":\n",
    "            choice = 1\n",
    "            parse_json = json.loads(str(df[6][i]))\n",
    "            if parse_json[\"Personal Information Type\"][\"endIndexInSegment\"] != -1:\n",
    "                total_data_samples1.append(parse_json[\"Personal Information Type\"][\"selectedText\"])\n",
    "                total_data_number1.append(1)\n",
    "                total_data_label1.append(\"Personal Information Type\")                \n",
    "                cnt1 = cnt1+1\n",
    "        if choice==0:\n",
    "#             print(\"practis is -->\", df[5][i])\n",
    "            parse_json = json.loads(str(df[6][i]))\n",
    "#             print(parse_json)\n",
    "#             print(len(df[6][i]))\n",
    "#             print(parse_json.keys())\n",
    "            attributes = parse_json.keys()\n",
    "#             print(attributes)\n",
    "            for k in attributes:\n",
    "#                 print(k)\n",
    "                if parse_json[k]['startIndexInSegment'] != -1:\n",
    "#                     print(parse_json[k]['selectedText'])\n",
    "                    total_data_samples3.append(parse_json[k]['selectedText'])\n",
    "                    total_data_number3.append(3)\n",
    "                    total_data_label3.append(\"None\")  \n",
    "                    cnt3 = cnt3+1\n",
    "                    \n",
    "print(sen0_pos,sen0_neg)\n",
    "print(sen1_pos,sen1_neg)\n",
    "print(sen2_pos,sen2_neg)\n",
    "\n",
    "# print(sentence_list)    \n",
    "print(\"---\")\n",
    "print(cnt0)\n",
    "print(cnt1)\n",
    "print(cnt2)\n",
    "print(cnt3)\n",
    "print(\"--\")\n",
    "print(cnt0+cnt1+cnt2+cnt3)\n",
    "print(len(total_data_samples0))\n",
    "print(len(total_data_samples1))\n",
    "print(len(total_data_samples2))\n",
    "print(len(total_data_samples3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
